---
title: "Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Lab 1"
author: "Peter Trenkwalder, Jesse Calvillo, Shiraz Chakraverty"
output: 
  pdf_document:
  toc: true
  number_sections: true
fontsize: 11pt
geometry: margin=1in
---
# Introduction
The following exploration of the provided risk analysis report and dataset for US shuttle launches is intended for the w271 course instructional team. The Challenger shuttle accident is known to have been caused by the failure of an o-ring which was meant to prevent combustible gas leakage. As the consequences of o-ring failure are severe, it is of interest to ascertain any statistical inference possible from the data available. 

The included dataset is comprised of 23 flights and associated temperatures, pressures and o-ring failure status. The data spans approximately six years in the early 1980's. In our analysis, no consideration was given to the time dimension. We are particularly interested in answering the following questions:

 * Does lower temperature result in higher probability of O-ring failure? how large is this effect?
 * Does high pressure result in higher probability of O-ring failure? how large is this effect?  
 
Before inferring any models, it will be important to explore each individual parameter to assess spread, skewness, distribution, range and location. We also review interactions among the variables as well as interactions between the variables and o-ring failure. Our EDA informs our predictive models.  In the end, we want to be able to simply recapture the big lessons learned in simple terms:  

  * Should the flight be rescheduled based on a specific reading of one or more of these two features?  
  * Which model best explains and supports our conclusion?  
  * Does our logic flow seamlessly?  
  
We aim to summarize the insights gained from our statistical analysis in order to aid important and timely decision making, especially immediately prior to future launches. With the lives on the line, we're particularly interested in the following:    
```{r, warning=FALSE,message=FALSE}
load.libraries <- c('knitr','gridExtra','ggplot2','Hmisc','stargazer','dplyr','binom')
sapply(load.libraries, require, character = TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, warning = FALSE,message = FALSE)
```
# Data Validation
Here is a brief description of the dataset given to us. For the purpose of our research questions, we don't find any data gaps yet. 
```{r}
shuttle <- read.csv(file  = "http://www.chrisbilder.com/categorical/Chapter2/challenger.csv")
paste0("1. Blank rows: ",sum(is.na(shuttle)),"2. Duplicate rows: ",nrow(shuttle) - nrow(unique(shuttle))) # black rows
glimpse(shuttle)
```
The dataset provided has 23 rows and 5 columns. We can visually confirm that there are no blanks, duplicates.  
# Exploratory Data Analysis  
### Univariate Analysis  
**Temperature analysis**: The mean temperature is almost 70 degrees, the minimum is 53(cold) and max is 81(hot), range of 28 degrees, only 50% flights flew above the mean. Notice clusters of flights from the 25th and 75th percentile range, this is the "middle" while there are "cold" and "hot" ends where flights took off.  
**Pressure Analysis**: 15 flights used 200 psi leak test pressure, while 2 used 100 and 6 used 50 psi. There are a lot of failures where 200 psi test pressure was used.   
**O.ring Analysis**: Total seven space flights have at least one o-ring failures, 2 have more than one failure. 16 flights had no failures. A failure of 30% is very high. Based on this data, we have can engineer a feature called 'Failure' to denote a 0 (non-failure) or 1 (failure), irrespective of quantity of o-ring that failed.   
```{r, fig.height=3,fig.width=8}
g1 <- ggplot(data = shuttle, aes(y = "", x = Temp, color = Temp)) +
  geom_boxplot() + geom_jitter() + ggtitle("Temperature") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  coord_flip() + scale_x_continuous(breaks=seq(45,85,3)) + 
  theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
g2 <- ggplot(data = shuttle, aes(y = "", x = Pressure, color = Pressure)) +
  geom_boxplot() + geom_jitter() + ggtitle("Pressure") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  coord_flip() + scale_x_continuous(breaks=seq(25,250,25)) + 
  theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
g3 <- ggplot(data = shuttle, aes(y = factor(O.ring), x = Flight, color = factor(O.ring))) +
  geom_boxplot() + geom_jitter() + ggtitle("O.ring Failure") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  coord_flip() + scale_x_continuous(breaks=seq(1,24,1)) + 
  theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
grid.arrange(g1,g2,g3,ncol=3)
```
```{r}
describe(shuttle$Temp)
describe(shuttle$Pressure)
describe(shuttle$O.ring)
```
# Bivariate Analysis  
### Temperature & O.ring  
When comparing temperature to o-ring failure, it is evident that failures tend to occur at lower temperatures (mean of approximately 63F, 7 failures) than in the higher range (mean of 71F, 0 failures). As there are only two flights with multiple o-ring failures, we can group them into the same failure category as the rest of the failed o-ring results to create a binary variable (where 0=non-failure and 1=failure). As shown below, the o-ring failure has a -0.51 correlation with temperature (as temperatures get “cooler”, o-ring failure tends to increase).  
```{r, fig.height=2.5,fig.width=6}
# BOXPLOT
l1 <- ggplot(data = shuttle, aes(y = factor(O.ring), x = Temp, color = factor(O.ring))) +
  geom_boxplot() + geom_jitter() + ggtitle("O ring Failure by Temperature") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  coord_flip() + scale_x_continuous(breaks=seq(45,85,3)) + 
  theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
shuttle$Failure <- factor(ifelse(shuttle$O.ring >=1, 1, 0))

l2 <- ggplot(data = shuttle, aes(y = factor(Failure), x = Temp, color = factor(Failure))) +
  geom_boxplot() + geom_jitter() + ggtitle("O ring Failure by Temperature") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  coord_flip() + scale_x_continuous(breaks=seq(45,85,3)) + scale_colour_manual(values = c("dark green","red")) + theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
grid.arrange(l1,l2,ncol=2)
```
```{r}
describe(shuttle$Failure)
cor(shuttle$Temp,shuttle$O.ring)
```
### Pressure & O.ring   
As pressures were present in the data at only three levels, we treated this as a categorical variable. 6 failures occurred at “high” (200psi) pressures while only 2 occurred at “low” pressures (50psi). If we look at pressures over all the flights, it seems that pressure ranges have increased over time.  
```{r, fig.height=2.5,fig.width=6}
x1 <- ggplot(data = shuttle, aes(y = factor(Failure), x = Pressure, color = factor(Failure))) +
  geom_boxplot() + geom_jitter() + ggtitle("O ring Failure by Pressure") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  coord_flip() + scale_x_continuous(breaks=seq(25,250,25)) + scale_colour_manual(values = c("dark green","red")) +
  theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
x2 <- ggplot(data = shuttle, aes(y = Flight, x = factor(Pressure), color = factor(Failure))) +
  geom_boxplot() + geom_jitter() + ggtitle("O ring Failure by flight for pressure") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  scale_y_continuous(breaks=seq(1,24,1)) + scale_colour_manual(values = c("dark green","red")) +
  theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
grid.arrange(x1,x2,ncol = 2)
```
O-ring failure has a 0.28 correlation with pressure (as pressure increases, o-ring failure increases). However, the correlation is much less significant than that of temperature.  Because of this, it may be more accurate to build a model with only temperature (excluding pressure) so that the model only describes significant contributors to outcome. While perhaps lowering residuals, adding variables which don’t significantly contribute to outcome may cause over-fitting and poor generalizability.  
```{r}
cor(shuttle$Pressure,shuttle$O.ring)
```
### Temperature & Pressure 
**Temperature vs Pressure**: Temperature and pressure seem to have no relationship (correlation = 0.03) other than that as pressure increases the range in temperature gets larger. However, as we saw above, temperature is the only significant variable in this dataset.  
**Temperature, Pressure & O-ring Failure**: It is evident that o-ring failures occur more at lower temperatures. However, since there are much more data at high pressures it is unclear whether or not an interaction of pressure and temperature could be a significant contributor to failure. There is 1 failure at “low” pressure. 

```{r, fig.height=2.5,fig.width=6}
e1 <- ggplot(data = shuttle, aes(y = Temp, x = factor(Pressure), color = factor(Pressure))) +
  geom_boxplot() + geom_jitter() + ggtitle("Pressure by Temperature") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  scale_y_continuous(breaks=seq(52,82,3)) + 
  theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
e2 <- ggplot(data = shuttle, aes(y = Temp, x = factor(Pressure), color = factor(Failure))) +
  geom_boxplot() + geom_jitter() + ggtitle("Pressure by Temperature (All Flights)") + 
  theme(plot.title = element_text(lineheight=1, face="bold")) +
  scale_y_continuous(breaks=seq(52,82,3)) + scale_colour_manual(values = c("dark green","red")) + theme( axis.line = element_line(colour = "darkred", size = 1, linetype = "solid"))
grid.arrange(e1,e2,ncol=2)
```
```{r}
cor(shuttle$Temp,shuttle$Pressure)
```
# Exploratory Data Analysis Conclusion  
* Temperature has a -0.51 correlation with o-ring failure; as temperature “cools”, failures increase.  
* Pressure has a 0.28 correlation with o-ring failure; as pressure increases, failures increase.  
* Dala et. al. reports that "a warm o-ring will seal a joint appropriately and a cold o-ring may not.” (p.5, paragraph 5).  
* Plots of temperature and o-ring failure clearly show many failures at low temperatures.  
* Plots of pressure and o-ring failure show o-ring failures at high pressures.  
* Temperature and pressure are not correlated. Notedly, Dalal et. al. report that higher pressure tests were introduced to test the robustness of the seal.  
* The incidents of failure decreases with increased launch day temperatures (as temperatures “cool”, more failures occur).  
 
# Model Development Step-by-Step  
In order to test further, we can build a binomial model (with number of trials = 6) to describe the conditional probability of failure given a $temp$ and $pressure$. For comparison, we can also build a distinct binary model which describes $failure$ to be either 0 or 1.  
**Note** : Model table and LTE analysis are at the end of this section to allow for side-by-side comparison.

## Binomial model  

**Model 1a)** (checking effect of both input variables, without interaction)  
Proportion of O.ring Failures ~ Temperature + Pressure   
Here if $\pi(temp,pressure)$ is the joint probability for a given $temp$ and $pressure$ and $O-ring$ is a binomial variable with $ n = 6$ and $\pi = \pi(temp,pressure)$. The basic assumption here is that each of the 6 trials are independent and thus each o-ring can fail (independent of each the others) with the same probability. The logistical regression model is described as follows:  
$$
log \left(\frac{\pi(temp,pressure)}{1-\pi(temp,pressure)} \right) = \beta_0 + \beta_1 \cdot temp + \beta_2 \cdot pressure = 2.52 -0.098 \cdot temp + 0.008 \cdot pressure
$$
The model is fitted using the MLE method and the residual deviation is 16.546 with 20 degrees of freedom and a P-value of 0.21. Standard Error for the intercept is 3.48 and $\beta_{1}$ is 0.045 with $\beta_{2}$ being 0.008. The p-value for $\beta_{1}$ is significant above 95%, while $\beta_{0}$ and $\beta_{2}$ are not. When we drop $pressure$ from the model, we find that our LRT shows a much higher residual deviance of 18.086 and a difference of 1.54. Collectively, this is a good indication that $pressure$ does not explain the $\pi$ as well as temperature. With a net change of 1.54, it is evident that pressure adds little to the explanatory power of the model.   

**Model 1b)** (Adjusted model, only taking into account temperature)  
Proportion of O.ring Failures ~ Temperature  
As noted above, we simply dropped pressure and we now have the following model with $\beta_{0}$ at 5.085 and $\beta_{1}$ at -0.116. We have a higher residual deviance of 18.086, but coefficient for temperature is significant at 95% with a Standard Error = 0.047. The AIC (Akaike Information Criterion) is lower at 35.647 in comparison to the Model 1a which has an AIC of 36.106. Of these two models, model 1b is a better we will likely test remaining models against this in terms of performance.  
$$
log \left(\frac{\pi(temp)}{1-\pi(temp)} \right) = \beta_0 + \beta_1 \cdot temp = 5.085 - 0.116 \cdot temp
$$
  
## Binary model  
  
For comparison, we can imagine that any o-ring failure is a failure of the system. That is, our previous model suffers from the somewhat impractical assumption of each trial being independent (where each joint is sealed by an o-ring). In order to build this model, we must modify our dataset to look at any number of o-ring failures as just a binary failure = 1 (when no o-rings fail, failure= 0). In this case, we are not concerned about individual trial independence. Here we start with the following model,  
**Model 2a)** (checking effect of both input variables, without interaction)  
Failure ~ Temperature + Pressure  
Looking at the following coefficients for this model, it is evident that pressure does not appear to have a strong effect on failure in this case either.   
$$
log \left(\frac{\pi(temp,pressure)}{1-\pi(temp,pressure)} \right) = \beta_0 + \beta_1 \cdot temp + \beta_2 \cdot pressure = 13.262 -0.229 \cdot temp + 0.010 \cdot pressure
$$
We can note here the residual deviance is 18.78 as compared to 20.315 for the model with only temperature. The p-value at 0.21 does not indicate significance. This tells us that our null hypothesis (pressure has a coefficient = 0) cannot be rejected. Using the annova() test, however, we can see that temperature has a strong effect on o-ring failure. Between these two models, we can safely conclude that pressure remains to have a very weak effect.  

**Model 2b)** (Adjusted model, only taking into account temperature)  
Failure ~ Temperature  
$$
log \left(\frac{\pi(temp)}{1-\pi(temp)} \right) = \beta_0 + \beta_1 \cdot temp = 15.043 -0.232 \cdot temp
$$
  
## Model Comparison  
Now, we can further make some final determinations by comparing all of the models. Although the binomial and binary models with only temperature are both very strong, we can see these differences: 
 * Binomial model has residual deviance for 18.086 and binary model has 20.315, an improvement of 2.229.
 * Binomial model has an AIC of 35.64 and binary model has AIC of 24.61, binary is lower by 11.03.
 * Binomial model has a probability of 1.3% or 0.013(Chi-sq dist.) to see a coeff for temp mode extreme as -0.116.
 * Binary model has a probability of 0.48% or 0.004804 (Chi-sq dist.) to see a coeff for temp mode extreme as -0.232.
Based on these, we conclude that the temperature only binary model demonstrates better statistical performance and is a more robust choice over the binomial model. 
```{r, warning=FALSE}
#Build models
fl.fit.bnl.t <- glm(O.ring/Number ~ Temp, weights = Number, family = binomial(link= "logit"), data = shuttle)
fl.fit.bnl.tp <- glm(O.ring/Number ~ Temp+ Pressure, weights = Number, family = binomial(link= "logit"), data = shuttle)
fl.fit.tp <- glm(Failure ~ Temp + Pressure, family = binomial(link= "logit"), shuttle)
fl.fit.t <- glm(Failure ~ Temp, family = binomial(link= "logit"), shuttle)
fl.fit.tp.int <- glm(Failure ~ Temp + Pressure + Temp:Pressure, family = binomial(link= "logit"), shuttle)
#summary(fl.fit.bnl.t)
#summary(fl.fit.t)
# Display models and LTE tests
stargazer(fl.fit.bnl.tp,fl.fit.bnl.t,fl.fit.tp,fl.fit.t,fl.fit.tp.int, type = 'text')
anova(fl.fit.bnl.t,fl.fit.bnl.tp, test = "Chisq")
anova(fl.fit.t,fl.fit.tp,fl.fit.tp.int, test = "Chisq")
car::Anova(fl.fit.t,test = "LR")
car::Anova(fl.fit.bnl.t, test = "LR")
```
# Answers to question 4 and 5  
**4a)** There are five required conditions for a binomial probability model, these are that the trials need to be identical, each trial must have only two possible outcomes, trials need to be independent and the probability of success must remain constant for each trial and finally that the total number of successes or failures is denoted by the random variable W or in this case the total number of O.rings. The binomial model requires these model assuptions as otherwise the probabilities would not be possible to calculate.  
The problem with this assuption is two fold. First the trials are not really independent. One o-ring failure might result in a catastrophic event that would influence the other o-rings. It is entirely possible that the other o-rings get stressed more as a result of other factors like joint location, distance from the hot combustible gasses etc. We can think of many reasons why their probabilities of failure may not be identical.  
**4b)** Estimate the logistic regression model using the explanatory variables in a linear form. We build a binomial logistical regression model here. This assessment has been completed in the model construction section.  
```{r, warning=FALSE}
#Build models
fl.fit.bnl.t <- glm(O.ring/Number ~ Temp, weights = Number, family = binomial(link= "logit"), data = shuttle)
fl.fit.bnl.tp <- glm(O.ring/Number ~ Temp+ Pressure, weights = Number, family = binomial(link= "logit"), data = shuttle)
# stargazer(fl.fit.bnl.t,fl.fit.bnl.tp,fl.fit.tp.int,type = "text")
# anova(fl.fit.t,fl.fit.tp,fl.fit.tp.int, test = "Chisq")
```
Here if $\pi(temp,pressure)$ is the probability per joint for a given $temp$ and $pressure$, with $O-ring$ being a binomial variable with $ n = 6$ and $\pi = \pi(temp,pressure)$. This type of a model is built on a basic assumption that each of the 6 trials are independent and thus each o-rings can fail independent of each other with the same probability. Hence the mathamatical notation for this logistical regression model is :  
$$
logit(\pi)  = \beta_0 + \beta_1 \cdot temp + \beta_2 \cdot pressure = 2.52 -0.098 \cdot temp + 0.008 \cdot pressure
$$
**4(c)** Perform LRTs to judge the importance of the explanatory variables in the model.  
To do LRT analysis we need two models. For our null hypothesis we need a base model that states that only temperature is important.  
$$
H_{0} : \beta_0 + \beta_1 \cdot temp = 5.085 - 0.116 \cdot temp
$$
The alternate hypothesis would be that temperature and pressure are both important.  
$$
H_{A} : \beta_0 + \beta_1 \cdot temp + \beta_2 \cdot pressure = 2.52 -0.098 \cdot temp + 0.008 \cdot pressure
$$
Now using the LRT method, we can simply compare these two models. The Alt hypothesis model is fitted using the MLE method and the residual deviation is 16.546 with 20 degrees of freedom with a P-value of 0.21. We also notice that the Std. Error for the intercept is 3.48, $\beta_{1}$ is 0.045 and $\beta_{2}$ is 0.008. We also notice that the p-value for beta1 is significant above 95%, while beta0 and beta2 are not. When we drop $pressure$ from the model, this is our null hypothesis model, we find that our LRT analysis shows a much higher residual deviance of 18.086, a difference of 1.54 with the alternate hypothesis model. We also notice that the model with temperature and pressure has a p-value of 0.21.   
All these collecively show us that $pressure$ does not have a p-value below 0.05 and hence we cannot reject the null hypothesis and the model with only temperature.In the null hypothesis model, we have $\beta_{0}$ as 5.085 and $\beta_{1}$ as -0.116. We already see that we have a higher residual deviance of 18.086, coefficient for temperature is significant at 95% with a stadard error = 0.047. Also the "Akaike Inf. Crit." is lower at 35.647 , as compared to the model including pressure at 36.106.  
$$
H_{0} : \beta_0 + \beta_1 \cdot temp = 5.085 - 0.116 \cdot temp
$$
H_0 Cannot be rejected, i.e. we can say that only temperature is the important explanatory variable in the model.   
4(d) The authors chose to remove Pressure from the model based on the LRTs. Based on your results, discuss why you think this was done. Are there any potential problems with removing this variable?  
From the above anova(), LRT test we can clearly see that the chi-square critical value for the model including pressure has not been acheived, i.e. it is well below 95%, where it should be less than 0.05, however it is about 0.22, which means that the probability of us having a value as extreme as this when the null hypothesis is true is 22%, so this is telling us that we have a very high chance of choosing a false negetive, i.e. falsely rejecting the null hypothesis, at 22%. Also we see that the incremental difference in the residual deviation is -1.54, which is negligible compared to the null hypothesis at 18.54. So the incremental is negetive here.  
Now the only remaining issue to think about is if there is an interaction that is strong. We have computed this model above and can safely confirm that there is a negligible incremental residual deviance and the p value is at 40%, almost double with the model including only pressure. Hence we are twice as confident that we can safely remove pressure as it has no interaction effect with temperature.  
5. Continuing Exercise 4, consider the simplified model, where pi is the probability of an O-ring failure.   
(a) Estimate the model.  
```{r}
#Build models
fl.fit.bnl.t <- glm(O.ring/Number ~ Temp, weights = Number, family = binomial(link= "logit"), data = shuttle)

```
(b) Construct two plots:  (1) pi vs. Temp and (2) Expected number of failures vs. Temp. Use a temp range of 31 degrees to 81 degrees on the x-axis even though the minimum temp in the data set was 53 degrees.  
```{r, fig.height=4,fig.width=8}
beta.null <- fl.fit.bnl.t$coefficients[[1]]
beta.temp <- fl.fit.bnl.t$coefficients[[2]]
par(mfrow=c(1,2))
#predict
curve(expr = exp(beta.null + beta.temp*x)/(1+exp(beta.null + beta.temp*x)), xlim=c(31,85),
       main="Prob of failure by temp",
       xlab = "temp(F)", ylab = expression(pi), cex.main = 0.8)

curve(expr = (exp(beta.null + beta.temp*x)/(1+exp(beta.null + beta.temp*x)))*6, xlim = c(31,85),
     main = "Expected # of failures by temp",
xlab = "temp(F)", ylab = "# of failures", cex.main = 0.8)


```
c.Include the 95% Wald confidence interval bands for pi on the plot. Why are the bands much wider for lower temperatures than for higher temperatures?  
```{r, fig.height=4,fig.width=7}
# defining function to calculate the upper and lower bounds of the wald interval
# which are returned as a list
ci.pi = function(newdata, mod.fit.obj, alpha) { 
  linear.pred = predict(object = mod.fit.obj, newdata = newdata, 
                        type = "link", se = TRUE) 
  CI.lin.pred.lower = linear.pred$fit - qnorm (p = 1 - alpha/2) * linear.pred$se 
  CI.lin.pred.upper = linear.pred$fit + qnorm (p = 1 - alpha /2) * linear.pred$se
  CI.pi.lower = exp(CI.lin.pred.lower)/ (1 + exp(CI.lin.pred.lower)) 
  CI.pi.upper = exp(CI.lin.pred.upper)/(1 + exp(CI.lin.pred.upper)) 
  list (lower = CI.pi.lower, upper = CI.pi.upper)
}
par(mfrow = c(1,1))
# plotting predicted pis over temperature
curve(expr = exp(beta.null + beta.temp*x)/(1+exp(beta.null + beta.temp*x)), xlim = c(31,81),
      main = "Probability of failure given temperature", 
      xlab =  "temperature (F)", ylab = expression(pi), cex.main=.8)
# calling the above function via curve and overlaying confidence intervals 
# on the estimated probability of failure
curve(expr = ci.pi(newdata = data.frame(Temp = x), mod.fit.obj = fl.fit.bnl.t, alpha = 0.05)$upper,
      col = "red", lty = "dotdash", add = TRUE, xlim = c(31,81))
curve(expr = ci.pi(newdata = data.frame(Temp = x), mod.fit.obj = fl.fit.bnl.t, alpha = 0.05)$lower,
      col = "red", lty = "dotdash", add = TRUE, xlim = c(31,81))
```
The bands are much wider for lower intervals because our sample data is skewed toward higher temperature observations. There are few observations of low temperatures so our margins of error will be much higher relative to those related to higher temperatures.  
d.The temperature was 31 degrees at launch for the Challenger in 1986. Estimate the probability of an O-ring failure using this temperature, and compute a corresponding confidence interval. Discuss what assumptions need to be made in order to apply the inference procedures.  
```{r}
#predicting the probability of an O-ring failure using our logistic regression model
pi_hat = predict(object = fl.fit.bnl.t, newdata = data.frame(Temp=31), type = "response")[[1]]
print(paste("the estimated probability of an O-ring failure at 31 degrees is ", round(pi_hat,4)))
```
Relative to the Wald interval, the Wilson interval does a much better job of adhering to the stated confidence interval when the probability of success is closer to 0 or 1 and the number of observations are few. Given that we have fewer than 40 observations in our sample, we'll use the more robust Wilson interval.  
```{r}
# stating variables to pass into confint function
trials = 6
f = pi_hat[[1]]*trials
alpha = .05
#calculating 1-alpha wilson confidence interval for a response with a binomial distribution
binom.confint(x=f,n=trials,conf.level = 1-alpha, methods = "wilson")
```
We're 95% confident that our true probability of failure lies between 0.42 and 0.96 for a temperature of 31 degrees. The assumptions that need to be made in order to apply the inference procedures are those under the logistic regression model:  

* The logistic regression model assumes that the log-odds of an observation y can be expressed as a linear function of the input variables x  
* Thedependent variable needs to be binary (and not ordinal); specifically, the conditional distribution of y given follows a Bernoulli distribution  
*Observations are independent of each other. In fact, the error term of the model needs to follow an independent and identically distributed random variable
*No perfect collinearity among the explanatory variables  
*Linearity assumption: linearity of independent variables and log-odds ratio  
e. Rather than using Wald or profile LR intervals for the probability of failure, Dalal et al. (1989) use a parametric bootstrap to compute intervals. Their process was to:   
	1) simulate a large number of data sets (n = 23 for each) from the estimated model of logit(pi) = beta0 + beta1Temp;   
	2) estimate new models for each data set, say logit(pi) = beta0 + beta1 Temp  
	3) compute pi at a specific temperature of interest. The authors used the 0.05 and 0.95 observed quantiles from the pi simulated distribution as their 90% confidence interval limits.   
Using the parametric bootstrap, compute 90% confidence intervals separately at temperatures of 31 and 72:  
```{r}
# define a parametric bootstrap function that takes a number of replications and a temperature 
# as arguments and computes a 90% confidence interval using the the predicted pis from our 
# logit.fit_temp model as the parameter in our bootstrap
boot_func = function(reps,t){
  #generate a variable (reps) number of temperature samples with replacement 
  smpls = data.frame(replicate(reps,sample(shuttle$Temp,replace = T)))
  
  # define a function to estimate the number of failures based on our log model and the sampled temperatures
  # this function returns a dataframe that includes sampled temperatures and the correspond estimated failures and trials
  fail_estimator = function(temps){
    pis = c()
    fails = c()
    #we're given from the original dataset there are 6 trials for each observation
    trials = rep(6, 23)
    for (temp in temps) {
      pi_hat = predict(object = fl.fit.bnl.t, 
                       newdata = data.frame(Temp=temp), 
                       type = "response")
      pis = c(pis,pi_hat)
      fails = c(fails,rbinom(1,6,pi_hat)) 
    }
    return(data.frame(temps,pis,fails,trials))
  }
  
  # iterate through each sample, calling the fail_estimator function to generate a new data set
  # upon which to run a logistic regression against and predict pi for a given temperature (t)

  est_pi_hats = c()
  
  for (smpl in smpls) {
    fail_est = fail_estimator(smpl)
    
    log.fit = glm(fails/trials ~ temps, 
        family = binomial(link = "logit"),weights = trials,
        data = fail_est)
    
    new.pi_hat = predict(object = log.fit, 
                  newdata = data.frame(temps = t), 
                  type = "response")
    est_pi_hats = c(est_pi_hats,new.pi_hat)
  }
  
  return(est_pi_hats)
}

#create an instance of our bootstrap function using 100 replications of datasets 
# for a temperature of 31 and 72 degrees
boot_call.31 = boot_func(100,31)
boot_call.72 = boot_func(100,72)
#extract the 5 and 95 percentils to get the upper and lower bounds of the 90% CI
q31 = quantile(boot_call.31,probs=c(.05,.95))
q31.lower = q31[[1]]
q31.upper = q31[[2]]
q72 = quantile(boot_call.72,probs=c(.05,.95))
q72.lower = q72[[1]]
q72.upper = q72[[2]]
```
Using the parametric bootstrap described above, we are 90% confident that the true probability of failure is between `r q31.lower` and `r q31.upper` for a temperature of 31 degrees. For a temperature of 72 degrees, we are 90% confident that the true probability of failure is between `r q72.lower` and `r q72.upper`.  
5(f) Determine if a quadratic term is needed in the model for the temperature.  
We can see that by adding a qudratic term our model either over estimates the probability at the lower end of temperature or underestimates in the middle and then again overestimates at the far right of temperature. hence we cannot justify use of a quadratic term.  
```{r, fig.height=4,fig.width=7}
fl.fit.bnl.t <- glm(O.ring/Number ~ Temp, weights = Number, family = binomial(link= "logit"), data = shuttle)
fl.fit.bnl.t.sq <- glm(O.ring/Number ~ Temp + I(Temp^2), weights = Number, family = binomial(link= "logit"), data = shuttle)
beta.null <- fl.fit.bnl.t$coefficients[[1]]
beta.temp <- fl.fit.bnl.t$coefficients[[2]]
beta.null.sq <- fl.fit.bnl.t.sq$coefficients[[1]]
beta.temp.sq <- fl.fit.bnl.t.sq$coefficients[[2]]
beta.temp2.sq <- fl.fit.bnl.t.sq$coefficients[[3]]
#predict
curve(expr = exp(beta.null + beta.temp*x)/(1+exp(beta.null + beta.temp*x)), xlim=c(31,85),
       main="Prob. of failure by temp", col = "blue", lty = 5,
       xlab = "temperature(F)", ylab = expression(pi), cex.main = 0.8)
curve(expr = exp(beta.null.sq + beta.temp.sq*x + beta.temp2.sq*x^2 )/(1 + exp(beta.null.sq + beta.temp.sq*x + beta.temp2.sq*x^2)), xlim=c(31,85),
       main="Prob. of failure by temp(as sq term)", col = "red", add = TRUE, lty = 10,
       xlab = "temperature(F)", ylab = expression(pi), cex.main = 0.8)
legend(x = 55, y = 0.52, legend = c("with sq term", "without sq term"), lty = c(2, 1), bty = "n", col=c("red", "blue"), cex = 1)
```
3. In addition to the questions in Question 4 and 5, answer the following questions:  
a. Interpret the main result of your final model in terms of both odds and probability of failure  
As we noted above in the model summary, we found the binary model to be the best, this model is noted as :  
$$
log \left(\frac{\pi(temp)}{1-\pi(temp)} \right) = \beta_0 + \beta_1 \cdot temp = 15.043 -0.232 \cdot temp
$$
The above expression siply means that if temperature decreases by 30 units, the odds of failure become  
$$
Odds_{(temp-c)}=exp( \beta_{0} + \beta_{1}( temperature - 30))
$$
In practical terms, we can also say that if the temperature was, The Odds ratio for this model means that the following would be the siplest way to compute the odds of failure when temperature decreases by 30 degrees.  
$$
OR = \frac{Odds_{temp-c}}{Odds_{temp}}=exp(c \beta_{temp})
$$
Using a few examples we illustrate ,  

 * In case the temperature decreases by 5 degrees, the odds of failure is 3.19 times more.
 * In case the temperature today is 50 degrees and yesterday it was 75 degrees, then compared to yesterday, the odds of failure today are 332 times more.  
This model is giving us he odds of failure and its interpretation is simple like a linear model, for a certain number of units of change we can compute the odds compared to a base number of units. This gives us the odds ratio.
```{r}
exp(-5*fl.fit.t$coefficients[2])
exp(-25*fl.fit.t$coefficients[2])

```
b. With the same set of explanatory variables in your final model, estimate a linear regression model. Explain the model results; conduct model diagnostic; and assess the validity of the model assumptions. Would you use the linear regression model or binary logistic  
regression in this case. Please explain.  
```{r,fig.height=5,fig.width=8, warning = FALSE}
fl.fit.lm <- lm(as.numeric(Failure) ~ Temp, data = shuttle)
summary(fl.fit.lm)
par(mfrow=c(2,2))
plot(fl.fit.lm)
```
Linearity in Parameters:  our model has linear relationship between the coefficients of temperature and the probability of failure    
Random Sample:  While we aren't given the time period of each flight's observation, it's likely that our observations are taken over time so we may have observations that are representative of different stages (groups) of flight technology and equipment, meaning our observations may not be random.    
No Perfect Collinearity:  Since we only have one explanatory variable in our model, perfect collinearity isn't an issue. Additionally, R automatically handles perfect collinearity between explanatory variables    
Zero Conditional Mean and Homoskedasticity:  From the residuals vs fitted values plot, we can see that the expected value (average) of the residuals veer from zero which indicates that our zero conditional mean assumption appears violated. This implies that there may be some omitted variable bias. Given that our model only includes 1 variable, it seems reasonable that there would be omitted variable bias.  
Homoskedasticity:  For better insight into potential heteroskedasticity, we can standardize the residuals and observe a scale-location plot of the fitted values. The scale-location plot implies we have heteroskedasticity in our variables. However, non-constant variance of residuals is a common problem in practice, which can be accomodated for this by using robust standar errors.  
Normality of Errors:  Based on the normal Q-Q plot, the Normality of Errors assumption appears to be upheld which allows us to draw inferences about the population, given other assumptions hold.  
Residuals vs Leverage: (Outlier Influence)  We check the influence of outliers on our model using the residuals vs leverage plot. Since the observations falls within a cook’s distance less than 1, we don't have any outliers that would influence a change in our regression results.  
CLM Summary:  Given the context of our goal in understanding the drivers behind the probability of an 0-ring failure, it doesn't make sense to use a linear model because linear models are unbound. In other words, a linear model would predict a probability of failure above 1 and/or below 0 which is impossible. Further, we see that our zero conditional mean and homoskedasticity assumptions are violated which leaves us with little faith that our model will generalize well to other datasets. Lastly, we don't believe we have random sampling as stated above. For these reasons, we will continue to use binary logistic regression and exclude linear regression from our modelling approach.   

# Final conclusion 
We find the binary model with only temperature as the most convincing to help us understand the o-ring failure rate:
$$
log \left(\frac{\pi(temp)}{1-\pi(temp)} \right) = \beta_0 + \beta_1 \cdot temp = 15.043 -0.232 \cdot temp
$$
 * **Does lower temperature result in higher probability of O-ring failure? How large is this effect?**  Yes, with this model, we can say for sure that as temperature decreases, the odds of failure increase. When temperature decreases by 5 degrees, the odds of failure is 3.19 times more.  In case the temperature today is 50 degrees and yesterday it was 75 degrees, then compared to yesterday, the odds of failure today are 332 times more. This model is giving us the odds of failure and its interpretation is simple like a linear model, for a certain number of units of change we can compute the odds compared to a base number of units. This gives us the odds ratio. 

 * **Does high pressure result in higher probability of O-ring failure? How large is this effect?** No, we did not find a strong relationship here and hence find that using pressure did not inform us enough. We conclude that pressure does not result in a higher probability that we are able to confidently measure.
 
In the end, we believe the flight should be rescheduled based on an assessment of the odds of failure based on temperature.

 * The binary logistical model with only temperature best explains the dataset and supports our conclusion.
 * As we saw evidence of several failures at low temperatures, we also noticed a high negetive correlation of failure with teperature. When we built a logistical regression model, we found coefficient for temperature and the intercept to be above the 95% confidence interval, meaning that there is less than 5% probability of seeing values more extreme than these when the hypothesis is true (i.e. that temperature has no effect on probability of failure). We proceeded to perform assessment of pressure and did not find these attributes.
